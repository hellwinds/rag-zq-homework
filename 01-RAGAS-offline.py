# 01-RAGAS-offline.py
import os
import logging
import json
import numpy as np
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import Faithfulness, AnswerRelevancy
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper

# è‡ªå®šä¹‰æœ¬åœ° LLM å’Œ Embeddingï¼ˆé€‚é… RAGASï¼‰
from langchain.llms.base import LLM
from langchain.embeddings.base import Embeddings
from transformers import AutoTokenizer, AutoModelForCausalLM
from milvus_model.dense import SentenceTransformerEmbeddingFunction
from sqlalchemy import create_engine, text
from typing import ClassVar  # â† æ–°å¢å¯¼å…¥
from ragas.run_config import RunConfig

run_config = RunConfig(
    max_workers=1,        # ç¦ç”¨å¤šçº¿ç¨‹
    timeout=300,          # å»¶é•¿è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
)

# æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# ==============================
# 1. æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆBGEï¼‰
# ==============================
class LocalBGEEmbeddings(Embeddings):
    def __init__(self, model_path: str):
        self.model = SentenceTransformerEmbeddingFunction(model_name=model_path, device="cpu")
    
    def embed_documents(self, texts):
        return self.model(texts)
    
    def embed_query(self, text):
        return self.model([text])[0]

# ==============================
# 2. æœ¬åœ° LLMï¼ˆQwen2.5-0.5Bï¼‰
# ==============================
class LocalQwenLLM(LLM):
    tokenizer: ClassVar = None
    model: ClassVar = None

    def __init__(self, model_path: str):
        super().__init__()
        if LocalQwenLLM.tokenizer is None:
            logging.info("[LLM] é¦–æ¬¡åŠ è½½ Qwen2.5-0.5B æ¨¡å‹...")
            LocalQwenLLM.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            LocalQwenLLM.model = AutoModelForCausalLM.from_pretrained(
                model_path,
                device_map="cpu",
                torch_dtype="auto",
                trust_remote_code=True
            )
    
    def _call(self, prompt: str, stop=None, run_manager=None, **kwargs) -> str:
        """
        å¿…é¡»æ”¯æŒ stop, run_manager, **kwargsï¼Œå¦åˆ™ RAGAS/LangChain ä¼šæŠ¥é”™
        """
        messages = [{"role": "user", "content": prompt}]
        text = LocalQwenLLM.tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        model_inputs = LocalQwenLLM.tokenizer([text], return_tensors="pt").to("cpu")
        generated_ids = LocalQwenLLM.model.generate(
            **model_inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=LocalQwenLLM.tokenizer.eos_token_id
        )
        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)]
        response = LocalQwenLLM.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
        return response.strip()

    @property
    def _llm_type(self):
        return "qwen2.5-0.5b"
# ==============================
# 3. åˆå§‹åŒ–ç»„ä»¶
# ==============================
logging.info("[åˆå§‹åŒ–] åŠ è½½æœ¬åœ°æ¨¡å‹...")
embedding_model = LocalBGEEmbeddings(r".\model")
llm_model = LocalQwenLLM(r".\qwen_1.5b")

# åŒ…è£…ä¸º RAGAS å…¼å®¹æ ¼å¼
llm = LangchainLLMWrapper(llm_model)
embeddings = LangchainEmbeddingsWrapper(embedding_model)

# ==============================
# 4. æ•°æ®åº“è¿æ¥ï¼ˆç”¨äºç”Ÿæˆ ground truthï¼‰
# ==============================
DB_URL = "mysql+pymysql://root:Rzxgh2025@10.128.12.214:3306/sakila"
engine = create_engine(DB_URL)

# ==============================
# 5. æ£€ç´¢å‡½æ•°ï¼ˆChromaï¼‰
# ==============================
import chromadb
chroma_client = chromadb.PersistentClient(path="./chroma_ddl_db")
ddl_collection = chroma_client.get_collection("ddl_knowledge")

q2sql_client = chromadb.PersistentClient(path="./chroma_q2sql_db")
q2sql_collection = q2sql_client.get_collection("q2sql_knowledge")

dbdesc_client = chromadb.PersistentClient(path="./chroma_dbdesc_db")
dbdesc_collection = dbdesc_client.get_collection("dbdesc_knowledge")

def retrieve_context(question: str):
    # æ£€ç´¢ä¸‰ç±»ä¸Šä¸‹æ–‡
    ddl_res = ddl_collection.query(query_texts=[question], n_results=3, include=["documents"])
    q2sql_res = q2sql_collection.query(query_texts=[question], n_results=3, include=["documents", "metadatas"])
    desc_res = dbdesc_collection.query(query_texts=[question], n_results=5, include=["documents", "metadatas"])
    
    ddl_ctx = "\n".join(ddl_res["documents"][0]) if ddl_res["documents"][0] else ""
    q2sql_ctx = "\n".join([
        f"NL: \"{doc}\"\nSQL: \"{meta.get('sql_text', '')}\""
        for doc, meta in zip(q2sql_res["documents"][0], q2sql_res["metadatas"][0])
    ]) if q2sql_res["documents"][0] else ""
    desc_ctx = "\n".join([
        f"{meta.get('table_name', '')}.{meta.get('column_name', '')}: {doc}"
        for doc, meta in zip(desc_res["documents"][0], desc_res["metadatas"][0])
    ]) if desc_res["documents"][0] else ""
    
    full_context = f"{ddl_ctx}\n{desc_ctx}\n{q2sql_ctx}".strip()
    return full_context.split("\n")  # RAGAS è¦æ±‚ list[str]

# ==============================
# 6. ç”Ÿæˆç­”æ¡ˆï¼ˆè°ƒç”¨æœ¬åœ° LLMï¼‰
# ==============================
def generate_answer(question: str, contexts: list) -> str:
    context_str = "\n".join(contexts)
    prompt = (
        f"### Schema Definitions:\n{context_str}\n"
        f"### Query:\n\"{question}\"\n"
        "è¯·åªè¿”å›SQLè¯­å¥ï¼Œä¸è¦åŒ…å«ä»»ä½•è§£é‡Šæˆ–è¯´æ˜ã€‚"
    )
    return llm_model._call(prompt)

# ==============================
# 7. æ„å»ºè¯„ä¼°æ•°æ®é›†
# ==============================
# ä» q2sql_pairs.json åŠ è½½æµ‹è¯•é—®é¢˜
with open("q2sql_pairs.json", "r", encoding="utf-8") as f:
    pairs = json.load(f)

questions = []
ground_truths = []
answers = []
contexts_list = []

for pair in pairs[:5]:  # å¯è°ƒæ•´æ•°é‡ï¼Œé¿å…è¯„ä¼°å¤ªæ…¢
    q = pair["question"]
    gt_sql = pair["sql"]
    
    # è·å–ä¸Šä¸‹æ–‡
    ctx = retrieve_context(q)
    
    # ç”Ÿæˆç­”æ¡ˆ
    ans = generate_answer(q, ctx)
    
    questions.append(q)
    ground_truths.append(gt_sql)
    answers.append(ans)
    contexts_list.append(ctx)

dataset = Dataset.from_dict({
    "question": questions,
    "answer": answers,
    "contexts": contexts_list,
    "ground_truth": ground_truths  # RAGAS å¯é€‰ï¼Œç”¨äº answer_correctnessï¼ˆæœ¬è„šæœ¬æœªç”¨ï¼‰
})

# ==============================
# 8. æ‰§è¡Œè¯„ä¼°
# ==============================
print("\n=== RAGAS ç¦»çº¿è¯„ä¼°ï¼ˆæœ¬åœ° BGE + æœ¬åœ° Qwenï¼‰ ===")

# Faithfulnessï¼ˆå¿ å®åº¦ï¼‰
print("\n[1/2] è¯„ä¼° Faithfulnessï¼ˆå¿ å®åº¦ï¼‰...")
faithfulness_metric = [Faithfulness(llm=llm)]
faithfulness_result = evaluate(dataset, faithfulness_metric, run_config=run_config)
faith_score = np.mean(faithfulness_result['faithfulness'])
print(f"âœ… Faithfulness: {faith_score:.4f}")

# AnswerRelevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰
print("\n[2/2] è¯„ä¼° AnswerRelevancyï¼ˆç­”æ¡ˆç›¸å…³æ€§ï¼‰...")
relevancy_metric = [AnswerRelevancy(llm=llm, embeddings=embeddings)]
relevancy_result = evaluate(dataset, relevancy_metric, run_config=run_config)
rel_score = np.mean(relevancy_result['answer_relevancy'])
print(f"âœ… AnswerRelevancy: {rel_score:.4f}")

print("\n=== è¯„ä¼°å®Œæˆ ===")
print(f"ç»¼åˆå¾—åˆ†: Faithfulness={faith_score:.4f}, Relevancy={rel_score:.4f}")

# å¯é€‰ï¼šä¿å­˜è¯¦ç»†ç»“æœ
import pandas as pd
df = dataset.to_pandas()
df["faithfulness"] = faithfulness_result["faithfulness"]
df["answer_relevancy"] = relevancy_result["answer_relevancy"]
df.to_csv("ragas_evaluation_offline_results.csv", index=False, encoding="utf-8")
print("\nğŸ“Š è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: ragas_evaluation_offline_results.csv")
